import numpy as np
from layers import *
import math
import pdb

def init_three_layer_neuralnet(weight_scale=0.001, bias_scale=0, input_feat_dim=786,
                           num_classes=10, num_neurons=(20, 30)):
  assert len(num_neurons)  == 2, 'You must provide number of neurons for two layers...'
  model = {}
  model['W1'] = weight_scale * np.random.randn(input_feat_dim, num_neurons[0]) * math.sqrt(2.0 / (input_feat_dim))# Initialize from a Gaussian With scaling of sqrt(2.0/fanin)
  model['b1'] = 0 # Initialize with zeros
  model['W2'] = weight_scale * np.random.randn(num_neurons[0], num_neurons[1]) * math.sqrt(2.0 / (num_neurons[0]))# Initialize from a Gaussian With scaling of sqrt(2.0/fanin)
  model['b2'] = 0 # Initialize with zeros
  model['W3'] = weight_scale * np.random.randn(num_neurons[1], num_classes) * math.sqrt(2.0 / (num_neurons[1]))# Initialize from a Gaussian With scaling of sqrt(2.0/fanin)
  model['b3'] = 0 # Initialize with zeros
  return model


def three_layer_neuralnetwork(X, model, y=None, reg=0.0,verbose=0):

  # Unpack weights
  W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']
  N,D= X.shape

  assert W1.shape[0] == D, ' W1 2nd dimenions must match number of features'
  
  dW1,dW2,dW3,db1,db2,db3=np.zeros_like(W1),np.zeros_like(W2),np.zeros_like(W3),np.zeros_like(b1),np.zeros_like(b2),np.zeros_like(b3)

  out_1, cache_1 = affine_forward(X, W1, b1)
  out_r_1, cache_r_1 = relu_forward(out_1)
  out_2, cache_2 = affine_forward(out_r_1, W2, b2)
  out_r_2, cache_r_2 = relu_forward(out_2)
  out_3, cache_3 = affine_forward(out_r_2, W3, b3)
  
  if verbose:
    print ['Layer {} Variance = {}'.format(i+1, np.var(l[:])) for i,l in enumerate([out_1, out_2, cache_3[0]])][:]
  if y is None:
    return out_3

  # Compute the backward pass
  data_loss, dx = softmax_loss(out_3, y)
  
  reg_loss = reg * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / 2.0 

  dx3, dW3, db3 = affine_backward(dx, cache_3)
  dx_r_2 = relu_backward(dx3, cache_r_2)
  dx2, dW2, db2 = affine_backward(dx_r_2, cache_2)
  dx_r_1 = relu_backward(dx2, cache_r_1)
  dx1, dW1, db1 = affine_backward(dx_r_1, cache_1)


  loss = data_loss + reg_loss
  grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2,'W3':dW3,'b3':db3}
  
  return loss, grads
